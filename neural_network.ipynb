import numpy as np

# Sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Input dataset: Features (SIFT, PolyPhen, LOEUF, pHaplo, Ptriplo)
X = np.array([
    [0.022, 0.850, 0.132298, 0.177497155, 0.215448019],  # iPSC
    [0.198, 0.612, 1.000000, 0.315614624, 0.341592185],  # hESC
    [0.480, 0.728, 0.480000, 0.347375626, 0.009351262],  # Fully Differentiated
    [0.835, 0.054, 0.250000, 0.916172973, 0.239416295],  # Somatic
    [0.682, 0.137, 0.970000, 0.942545412, 0.242846426]   # Cancer
])

# Target values (Example: Binary classification of cell type)
y = np.array([[0], [0], [1], [1], [1]])  # Labels (customize based on task)

# Set up the neural network architecture
input_layer_neurons = X.shape[1]  # Number of features in input layer (5)
hidden_layer_neurons = 4           # Number of neurons in hidden layer
output_layer_neurons = 1           # Number of neurons in output layer

# Initialize weights and biases with random values
np.random.seed(1)

weights_input_hidden = np.random.rand(input_layer_neurons, hidden_layer_neurons)
bias_hidden = np.random.rand(1, hidden_layer_neurons)

weights_hidden_output = np.random.rand(hidden_layer_neurons, output_layer_neurons)
bias_output = np.random.rand(1, output_layer_neurons)

# Learning rate
learning_rate = 0.1

# Training the network using backpropagation
epochs = 10000  # Number of iterations

for epoch in range(epochs):
    # Forward propagation
    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    predicted_output = sigmoid(output_layer_input)
    
    # Calculate error (Mean Squared Error)
    error = y - predicted_output
    
    # Backpropagation
    output_layer_error = error * sigmoid_derivative(predicted_output)
    hidden_layer_error = output_layer_error.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_layer_output)
    
    # Update weights and biases
    weights_hidden_output += hidden_layer_output.T.dot(output_layer_error) * learning_rate
    bias_output += np.sum(output_layer_error, axis=0, keepdims=True) * learning_rate
    
    weights_input_hidden += X.T.dot(hidden_layer_error) * learning_rate
    bias_hidden += np.sum(hidden_layer_error, axis=0, keepdims=True) * learning_rate
    
    # Print the error at every 1000th epoch to monitor progress
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Error: {np.mean(np.abs(error))}")

# Final predicted output after training
print("Predicted Output after training:")
print(predicted_output)
